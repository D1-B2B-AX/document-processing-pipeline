from fastapi import APIRouter, UploadFile, File, HTTPException
from pptx import Presentation
from openai import OpenAI
import io
import re
import os

client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
router = APIRouter()

# =========================================================
# [í‚¤ì›Œë“œ ì„¤ì •] (ì‚¬ìš©ìë‹˜ì˜ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤)
# =========================================================
EXCLUDE_KEYWORDS = [
    "ìœ ì‚¬", "ì‚¬ë¡€", "ì‹¤ì ", "reference", "case", "history", "result",
    "ê°•ì‚¬í”„ë¡œí•„", "ìˆ˜í–‰ì‹¤ì ", "ì œì•ˆì‚¬", "íšŒì‚¬ì†Œê°œ"
]

OVERVIEW_KEYWORDS = [
    "ê³¼ì • ì†Œê°œ", "ê³¼ì •ì†Œê°œ", "ê³¼ì • ê°œìš”", "ê³¼ì •ê°œìš”", 
    "êµìœ¡ ì†Œê°œ", "êµìœ¡ì†Œê°œ", "êµìœ¡ ê°œìš”", "êµìœ¡ê°œìš”", 
    "ê°œìš”", "ì†Œê°œ", "overview", "summary", "ìš”ì•½", "ì œì•ˆ ë°°ê²½", "ê¸°íš ì˜ë„",
    "ëª©í‘œ", "ëŒ€ìƒ" 
]

CURRICULUM_KEYWORDS = [
    "ì»¤ë¦¬í˜ëŸ¼", "ì„¸ë¶€ê³¼ì •", "êµìœ¡ê³¼ì •", "êµìœ¡ë‚´ìš©", "ëª¨ë“ˆêµ¬ì„±", 
    "ìƒì„¸ê³¼ì •", "í”„ë¡œê·¸ë¨", "module", "schedule", "curriculum",
    "ëª¨ë“ˆ", "êµ¬ì„±", "ì¼ì •", "ë°©ë²•", "contents", "agenda", "syllabus",
    "1ì¼ì°¨", "2ì¼ì°¨", "1h", "2h", "time" 
]

# =========================================================
# [ê¸°ëŠ¥ 1] í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì¬ê·€ + Duck Typing)
# í…ìŠ¤íŠ¸ê°€ ê·¸ë£¹(Group) ì•ˆì— ìˆì–´ë„ ë¬´ì¡°ê±´ êº¼ë‚´ëŠ” í•µì‹¬ ë¡œì§ì…ë‹ˆë‹¤.
# =========================================================
def normalize(text):
    return re.sub(r'\s+', '', str(text).lower())

def get_text_from_shape_recursive(shape):
    """ë„í˜•, í‘œ, ê·¸ë£¹ ë‚´ë¶€ë¥¼ ê°€ë¦¬ì§€ ì•Šê³  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    text_parts = []
    try:
        # 1. í…ìŠ¤íŠ¸ ë°•ìŠ¤
        if hasattr(shape, "text") and shape.text and shape.text.strip():
            text_parts.append(shape.text.strip())
        
        # 2. í‘œ (Table)
        if hasattr(shape, "table") and shape.table:
            for row in shape.table.rows:
                row_cells = [c.text.replace('\n', ' ').strip() for c in row.cells if c.text.strip()]
                if row_cells:
                    text_parts.append(f"| {' | '.join(row_cells)} |")
        
        # 3. ê·¸ë£¹ (ì¬ê·€ íƒìƒ‰) - ì‚¬ìš©ìë‹˜ì´ ì›í•˜ì‹  ì¬ê·€ ë¡œì§ ì ìš©
        if hasattr(shape, "shapes"):
            for child in shape.shapes:
                text_parts.extend(get_text_from_shape_recursive(child))
    except:
        pass
    return text_parts

def extract_text_from_slide(slide):
    """ìŠ¬ë¼ì´ë“œ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    all_texts = []
    
    # ì œëª© ì²˜ë¦¬ (Visual Title ë¡œì§ ëŒ€ì‹ , ì•ˆì „í•˜ê²Œ ê°ì²´ ì†ì„± í™•ì¸)
    try:
        if slide.shapes.title and slide.shapes.title.text.strip():
            all_texts.append(f"### {slide.shapes.title.text.strip()}")
    except:
        pass 

    # ë³¸ë¬¸ ì²˜ë¦¬ (ì œëª© ì œì™¸)
    for shape in slide.shapes:
        try:
            if slide.shapes.title and shape == slide.shapes.title:
                continue
        except:
            pass
        all_texts.extend(get_text_from_shape_recursive(shape))
        
    return "\n".join(all_texts)

# =========================================================
# [ê¸°ëŠ¥ 2] ìŠ¬ë¼ì´ë“œ ë¶„ë¥˜ (ì‚¬ìš©ì ë¡œì§ + ë‚´ìš© ê¸°ë°˜ ë³´ì™„)
# =========================================================
def classify_slide_by_content(full_text):
    """
    ì œëª© ìœ„ì¹˜(Top)ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë³´ê³  ë¶„ë¥˜í•©ë‹ˆë‹¤.
    (ê·¸ë£¹ ì•ˆì— ì œëª©ì´ ìˆ¨ì–´ìˆì„ ë•Œë„ ì‘ë™í•˜ê¸° ìœ„í•¨)
    """
    norm_text = normalize(full_text)
    
    for key in EXCLUDE_KEYWORDS:
        if normalize(key) in norm_text: return "EXCLUDE"
    for key in OVERVIEW_KEYWORDS:
        if normalize(key) in norm_text: return "OVERVIEW"
    for key in CURRICULUM_KEYWORDS:
        if normalize(key) in norm_text: return "CURRICULUM"
    
    return "OTHER"

# =========================================================
# [ê¸°ëŠ¥ 3] LLM ë³€í™˜ (í‘œ ê°•ì œ + ì‹œê°„ ë³´ì¡´)
# =========================================================
def generate_rag_markdown(filename, course_idx, overview_text, curriculum_text):
    if len(curriculum_text) < 30: return None

    # í† í° ì œí•œ ì•ˆì „ì¥ì¹˜
    safe_curriculum = curriculum_text[:20000]

    prompt = f"""
    ë‹¹ì‹ ì€ 'ê¸°ì—… êµìœ¡ ì œì•ˆì„œ ë¶„ì„ ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.
    ì œê³µëœ Raw Textë¥¼ ë¶„ì„í•˜ì—¬ RAG ê²€ìƒ‰ì— ìµœì í™”ëœ **Clean Markdown** í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ì‹­ì‹œì˜¤.

    [Input Source]
    - File: {filename}
    - Context (ê°œìš”): {overview_text[:3000]}
    - Content (ì»¤ë¦¬í˜ëŸ¼): {safe_curriculum}

    [Output Rules - Strict]
    1. **Metadata**: ë¬¸ì„œ ìµœìƒë‹¨ì— `> **Keywords**: ...` í˜•ì‹ìœ¼ë¡œ í•µì‹¬ í‚¤ì›Œë“œ(ëŒ€ìƒ, ì£¼ì œ, íˆ´ ë“±) ë‚˜ì—´.
    2. **Table Formatting (í•„ìˆ˜)**: 
       - ì»¤ë¦¬í˜ëŸ¼ì˜ ìƒì„¸ ì¼ì •, ëª¨ë“ˆ êµ¬ì„±ì€ **ë°˜ë“œì‹œ Markdown Table**ë¡œ ì‘ì„±í•  ê²ƒ.
       - ì˜ˆì‹œ: | ëª¨ë“ˆëª… | ì‹œê°„ | ì£¼ìš” ë‚´ìš© | êµìœ¡ ë°©ë²• |
    3. **Time Preservation**: '1H', '2ì‹œê°„', '09:00~18:00' ë“± ì‹œê°„ ì •ë³´ëŠ” **ì ˆëŒ€ ì‚­ì œ ê¸ˆì§€**.
    4. **Filtering**: ê°•ì‚¬ í”„ë¡œí•„, íšŒì‚¬ í™ë³´ ë“± ì»¤ë¦¬í˜ëŸ¼ê³¼ ë¬´ê´€í•œ ë‚´ìš©ì€ ì‚­ì œ.
    5. **No Chit-chat**: ì„œë¡  ì—†ì´ ê²°ê³¼ Markdownë§Œ ì¶œë ¥.
    """

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"LLM Error: {e}")
        return None

# =========================================================
# [Endpoint] ë©”ì¸ í•¸ë“¤ëŸ¬
# =========================================================
@router.post("/parse")
async def parse_curriculum(file: UploadFile = File(...)):
    print(f"ğŸš€ Processing: {file.filename}")
    
    content = await file.read()
    try:
        prs = Presentation(io.BytesIO(content))
    except Exception as e:
        raise HTTPException(status_code=400, detail="Invalid PPTX file")

    courses = [] 
    current_course = {'overview': [], 'curriculum': []}
    
    # 1. ìŠ¬ë¼ì´ë“œ ìˆœíšŒ (ì‚¬ìš©ìë‹˜ì˜ ë¡œì§ ë°˜ì˜: OVERVIEWë§ˆë‹¤ ê³¼ì • ë¶„ë¦¬)
    for i, slide in enumerate(prs.slides):
        # ì¬ê·€í•¨ìˆ˜ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê·¸ë£¹ ë‚´ë¶€ í¬í•¨)
        full_text = extract_text_from_slide(slide)
        
        # ë‚´ìš© ê¸°ë°˜ ë¶„ë¥˜
        slide_type = classify_slide_by_content(full_text)
        
        if slide_type == "EXCLUDE": 
            continue

        if slide_type == "OVERVIEW":
            # [ì¤‘ìš”] ìƒˆë¡œìš´ ê°œìš”ê°€ ë‚˜ì˜¤ë©´ ì´ì „ ê³¼ì •ì„ ì €ì¥í•˜ê³  ë¦¬ì…‹ (ì‚¬ìš©ì ë¡œì§)
            if current_course['curriculum']: 
                courses.append(current_course)
                current_course = {'overview': [], 'curriculum': []}
            current_course['overview'].append(full_text)

        elif slide_type == "CURRICULUM":
            current_course['curriculum'].append(full_text)
            
        # ë¶„ë¥˜ê°€ ì•ˆ ëœ ìŠ¬ë¼ì´ë“œ(OTHER)ë¼ë„ í…ìŠ¤íŠ¸ê°€ ê¸¸ë©´ ì»¤ë¦¬í˜ëŸ¼ìœ¼ë¡œ ê°„ì£¼ (ì•ˆì „ì¥ì¹˜)
        elif slide_type == "OTHER" and len(full_text) > 50:
             current_course['curriculum'].append(full_text)

    # ë§ˆì§€ë§‰ì— ë‚¨ì€ ê³¼ì • ì¶”ê°€
    if current_course['curriculum']:
        courses.append(current_course)

    print(f"ğŸ“Š ê°ì§€ëœ ê³¼ì •(Courses) ìˆ˜: {len(courses)}ê°œ")

    # 2. LLM ë³€í™˜ ë° ê²°ê³¼ ìƒì„±
    results = []
    for idx, course in enumerate(courses):
        full_overview = "\n\n".join(course['overview'])
        full_curriculum = "\n\n".join(course['curriculum'])
        
        md_content = generate_rag_markdown(file.filename, idx+1, full_overview, full_curriculum)
        
        if md_content:
            # [ìˆ˜ì •ë¨] íŒŒì¼ëª… ìƒˆë‹ˆíƒ€ì´ì§•(íŠ¹ìˆ˜ë¬¸ì ì œê±°) ë¡œì§ ì‚­ì œ -> ì›ë³¸ íŒŒì¼ëª… ìœ ì§€
            base_name = os.path.splitext(file.filename)[0]
            
            # ê³¼ì •ì´ ì—¬ëŸ¬ ê°œì¼ ë•Œë§Œ ë’¤ì— ë²ˆí˜¸ ë¶™ì„, í•˜ë‚˜ë©´ ê¹”ë”í•˜ê²Œ ì›ë³¸ëª… ì‚¬ìš©
            suffix = f"_Course_{idx+1}" if len(courses) > 1 else "_Parsed"
            suggested_filename = f"{base_name}{suffix}.md"
            
            results.append({
                "course_index": idx + 1,
                "suggested_filename": suggested_filename,
                "markdown": md_content
            })

    return {
        "domain": "curriculum",
        "original_filename": file.filename,
        "parsed_courses": results,
        "count": len(results)
    }